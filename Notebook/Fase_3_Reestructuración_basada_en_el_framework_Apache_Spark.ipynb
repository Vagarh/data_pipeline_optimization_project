{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preparacion del entrono de trabajo"
      ],
      "metadata": {
        "id": "DTkVWusrOpcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento 1 Prepacion del entorno"
      ],
      "metadata": {
        "id": "ViBnjfBAcUIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Apache Spark y Hadoop\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Descargar Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Verificar que el archivo se descargó\n",
        "!ls -lh\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Nws_rmTiOR",
        "outputId": "7f22cc93-bcc0-49c8-9f67-a63f46b49dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 384M\n",
            "-rw-r--r--  1 root root  584 Oct 23 16:08 analysis.py\n",
            "-rw-r--r--  1 root root 1.3M Oct 23 16:09 Films_2.xlsx\n",
            "drwxr-xr-x  1 root root 4.0K Oct 21 13:22 sample_data\n",
            "drwxr-xr-x 13 1000 1000 4.0K Sep  9  2023 spark-3.5.0-bin-hadoop3\n",
            "-rw-r--r--  1 root root 382M Sep  9  2023 spark-3.5.0-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "# Configurar las rutas de Spark y Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "# Iniciar Spark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "JZsectg3UDcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Iniciar la sesión de Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Prueba_Spark\").getOrCreate()\n",
        "\n",
        "# Verificar que Spark esté activo\n",
        "print(\"Versión de Spark:\", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMHgHBcRcpn5",
        "outputId": "64f4258f-3318-4c23-b691-41560a880f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de Spark: 3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## DataFrame de prueba\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Crear un DataFrame de ejemplo con datos simples\n",
        "data = [Row(nombre=\"Juan\", edad=28),\n",
        "        Row(nombre=\"Ana\", edad=24),\n",
        "        Row(nombre=\"Pedro\", edad=30)]\n",
        "\n",
        "# Convertir la lista de datos a un DataFrame de Spark\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Mostrar el contenido del DataFrame\n",
        "df.show()\n",
        "\n",
        "# Hacer una operación básica: contar el número de filas\n",
        "print(f\"Total de filas en el DataFrame de prueba: {df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzAYJHJ-cvlZ",
        "outputId": "91e9d19c-795f-4ea4-bfa5-b21ad47daa8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+\n",
            "|nombre|edad|\n",
            "+------+----+\n",
            "|  Juan|  28|\n",
            "|   Ana|  24|\n",
            "| Pedro|  30|\n",
            "+------+----+\n",
            "\n",
            "Total de filas en el DataFrame de prueba: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Operaciones adicionales de prueba\n",
        "\n",
        "# Filtrar los datos donde la edad es mayor que 25\n",
        "df.filter(df.edad > 25).show()\n",
        "\n",
        "# Agrupar por un valor (en este caso no tiene sentido porque son pocos datos, pero sirve de prueba)\n",
        "df.groupBy(\"nombre\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_tqsF6Nc13u",
        "outputId": "2c760fc5-2818-4154-bcf9-d6c049f1a561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+\n",
            "|nombre|edad|\n",
            "+------+----+\n",
            "|  Juan|  28|\n",
            "| Pedro|  30|\n",
            "+------+----+\n",
            "\n",
            "+------+-----+\n",
            "|nombre|count|\n",
            "+------+-----+\n",
            "|  Juan|    1|\n",
            "| Pedro|    1|\n",
            "|   Ana|    1|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openpyxl  # Para manejar archivos Excel\n",
        "!pip install -q pandas  # pandas también es necesario"
      ],
      "metadata": {
        "id": "5_CHSs6DdJkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar Spark\n",
        "findspark.init()\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Excel_Spark\").getOrCreate()\n",
        "\n",
        "# Verificar que Spark esté funcionando correctamente\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0RvbMvPVdsDv",
        "outputId": "38e8cd17-ec2b-4e66-d36e-a0aa259be8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d7558a4a9e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://83bdef5892e7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Prueba_Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiento 2 importando xls films"
      ],
      "metadata": {
        "id": "LF7FJdonzsB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear SparkSession (en lugar de SQLContext)\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ExcelToSpark\").getOrCreate()\n",
        "\n",
        "# Leer el archivo Excel con pandas (todas las hojas)\n",
        "excel_file = \"/content/Films_2.xlsx\"  # Cambia esto por el nombre de tu archivo subido\n",
        "sheets_dict = pd.read_excel(excel_file, sheet_name=None)  # Cargar todas las hojas\n",
        "\n",
        "# Verificar que las hojas se hayan cargado correctamente\n",
        "print(f\"Hojas cargadas: {sheets_dict.keys()}\")\n",
        "\n",
        "# Convertir cada hoja de pandas a DataFrame de Spark y guardarlo en un diccionario\n",
        "spark_dfs = {}\n",
        "for sheet_name, df in sheets_dict.items():\n",
        "    if df.shape[0] > 0:  # Verificar si la hoja no está vacía\n",
        "        spark_dfs[sheet_name] = spark.createDataFrame(df)\n",
        "        print(f\"Hoja '{sheet_name}' convertida a DataFrame de Spark con {df.shape[0]} filas.\")\n",
        "    else:\n",
        "        print(f\"La hoja '{sheet_name}' está vacía y no se convertirá.\")\n",
        "\n",
        "# Mostrar los nombres de las hojas convertidas y el número de filas en cada DataFrame de Spark\n",
        "for sheet_name, df in spark_dfs.items():\n",
        "    print(f\"Hoja: {sheet_name}, Número de filas: {df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slNNi5MYd1in",
        "outputId": "262980ac-047e-429d-961f-b592f007dbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hojas cargadas: dict_keys(['MER', 'film', 'inventory', 'rental', 'customer', 'store'])\n",
            "La hoja 'MER' está vacía y no se convertirá.\n",
            "Hoja 'film' convertida a DataFrame de Spark con 1003 filas.\n",
            "Hoja 'inventory' convertida a DataFrame de Spark con 4581 filas.\n",
            "Hoja 'rental' convertida a DataFrame de Spark con 16044 filas.\n",
            "Hoja 'customer' convertida a DataFrame de Spark con 1392 filas.\n",
            "Hoja 'store' convertida a DataFrame de Spark con 2 filas.\n",
            "Hoja: film, Número de filas: 1003\n",
            "Hoja: inventory, Número de filas: 4581\n",
            "Hoja: rental, Número de filas: 16044\n",
            "Hoja: customer, Número de filas: 1392\n",
            "Hoja: store, Número de filas: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ExcelToSpark\").getOrCreate()\n",
        "\n",
        "# Leer el archivo Excel con pandas (todas las hojas)\n",
        "excel_file = \"/content/Films_2.xlsx\"  # Cambia esto por el nombre de tu archivo subido\n",
        "sheets_dict = pd.read_excel(excel_file, sheet_name=None)  # Cargar todas las hojas\n",
        "\n",
        "# Verificar que las hojas se hayan cargado correctamente\n",
        "print(f\"Hojas cargadas: {sheets_dict.keys()}\")\n",
        "\n",
        "# Convertir cada hoja de pandas a DataFrame de Spark y guardarlo en un diccionario\n",
        "spark_dfs = {}\n",
        "for sheet_name, df in sheets_dict.items():\n",
        "    if df.shape[0] > 0:  # Verificar si la hoja no está vacía\n",
        "        spark_dfs[sheet_name] = spark.createDataFrame(df)\n",
        "        print(f\"Hoja '{sheet_name}' convertida a DataFrame de Spark con {df.shape[0]} filas.\")\n",
        "    else:\n",
        "        print(f\"La hoja '{sheet_name}' está vacía y no se convertirá.\")\n",
        "\n",
        "# Mostrar los nombres de las hojas convertidas y el número de filas en cada DataFrame de Spark\n",
        "for sheet_name, df in spark_dfs.items():\n",
        "    print(f\"Hoja: {sheet_name}, Número de filas: {df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj6Ip5__lRDa",
        "outputId": "006f3c97-593f-4541-da3c-640fa9ee4117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hojas cargadas: dict_keys(['MER', 'film', 'inventory', 'rental', 'customer', 'store'])\n",
            "La hoja 'MER' está vacía y no se convertirá.\n",
            "Hoja 'film' convertida a DataFrame de Spark con 1003 filas.\n",
            "Hoja 'inventory' convertida a DataFrame de Spark con 4581 filas.\n",
            "Hoja 'rental' convertida a DataFrame de Spark con 16044 filas.\n",
            "Hoja 'customer' convertida a DataFrame de Spark con 1392 filas.\n",
            "Hoja 'store' convertida a DataFrame de Spark con 2 filas.\n",
            "Hoja: film, Número de filas: 1003\n",
            "Hoja: inventory, Número de filas: 4581\n",
            "Hoja: rental, Número de filas: 16044\n",
            "Hoja: customer, Número de filas: 1392\n",
            "Hoja: store, Número de filas: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los primeros registros de la hoja 'Customer'\n",
        "if 'customer' in spark_dfs:\n",
        "    print(\"Mostrando los primeros registros de la tabla Customer:\")\n",
        "    spark_dfs['customer'].show(5)  # Cambia el número de filas a mostrar si es necesario\n",
        "else:\n",
        "    print(\"La hoja 'Customer' no está disponible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs6q7mQHlKiC",
        "outputId": "1059da48-6ee2-404f-d7b4-74871dc5e5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mostrando los primeros registros de la tabla Customer:\n",
            "+-----------+---------+-----------+----------+--------------------+-----------+-------+--------------------+--------------------+----------------+--------+\n",
            "|customer_id| store_id| first_name| last_name|               email| address_id| active|         create_date|         last_update| customer_id_old| segment|\n",
            "+-----------+---------+-----------+----------+--------------------+-----------+-------+--------------------+--------------------+----------------+--------+\n",
            "|          1|        1|       MARY|     SMITH| MARY.SMITH@sakil...|          5|      1| 2006-02-14 22:04:36| 2006-02-15 04:57:20|            NULL|    NULL|\n",
            "|          2|        1|   PATRICIA|   JOHNSON| PATRICIA.JOHNSON...|          6|      1| 2006-02-14 22:04:36| 2006-02-15 04:57:20|            NULL|    NULL|\n",
            "|          3|        1|      LINDA|  WILLIAMS| LINDA.WILLIAMS@s...|          7|      1| 2006-02-14 22:04:36| 2006-02-15 04:57:20|            NULL|    NULL|\n",
            "|          4|        2|    BARBARA|     JONES| BARBARA.JONES@sa...|          8|      1| 2006-02-14 22:04:36| 2006-02-15 04:57:20|            NULL|    NULL|\n",
            "|          5|        1|  ELIZABETH|     BROWN| ELIZABETH.BROWN@...|          9|      1| 2006-02-14 22:04:36| 2006-02-15 04:57:20|            NULL|    NULL|\n",
            "+-----------+---------+-----------+----------+--------------------+-----------+-------+--------------------+--------------------+----------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los primeros registros de la hoja 'Customer'\n",
        "if 'film' in spark_dfs:\n",
        "    print(\"Mostrando los primeros registros de la tabla Customer:\")\n",
        "    spark_dfs['film'].show(5)  # Cambia el número de filas a mostrar si es necesario\n",
        "else:\n",
        "    print(\"La hoja 'film' no está disponible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbm1A1kizgwm",
        "outputId": "38457dc7-c740-40fe-8777-7520f4ee4bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mostrando los primeros registros de la tabla Customer:\n",
            "+-------+-----------------+--------------------+-------------+------------+---------------------+----------------+------------+-------+-----------------+----------------+-------+-----------------+--------------------+\n",
            "|film_id|            title|         description| release_year| language_id| original_language_id| rental_duration| rental_rate| length| replacement_cost| num_voted_users| rating| special_features|         last_update|\n",
            "+-------+-----------------+--------------------+-------------+------------+---------------------+----------------+------------+-------+-----------------+----------------+-------+-----------------+--------------------+\n",
            "|      1| ACADEMY DINOSAUR| A Epic Drama of ...|         2006|           1|                 NULL|               6|        0.99|     86|            20.99|           76750|     PG|   Deleted Scenes| 2020-01-25 14:40:46|\n",
            "|      2|   ACE GOLDFINGER| A Astounding Epi...|         2006|           1|                 NULL|               3|        4.99|     48|            12.99|           19350|      G|         Trailers| 2020-01-25 14:40:46|\n",
            "|      3| ADAPTATION HOLES| A Astounding Ref...|         2006|           1|                 NULL|               7|        2.99|     50|            18.99|           20700|  NC-17|         Trailers| 2020-01-25 14:40:46|\n",
            "|      4| AFFAIR PREJUDICE| A Fanciful Docum...|        x2006|           1|                 NULL|               5|        2.99|    117|            26.99|           45500|      G|     Commentaries| 2020-01-25 14:40:46|\n",
            "|      5|      AFRICAN EGG| A Fast-Paced Doc...|         2006|           1|                 NULL|               6|        2.99|    130|            22.99|           11300|      G|   Deleted Scenes| 2020-01-25 14:40:46|\n",
            "+-------+-----------------+--------------------+-------------+------------+---------------------+----------------+------------+-------+-----------------+----------------+-------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento 3 codigo depurado aplciado"
      ],
      "metadata": {
        "id": "Ea7IHugIcg30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, regexp_replace, to_timestamp, trim\n",
        "from pyspark.sql.types import IntegerType, FloatType\n",
        "import pandas as pd\n",
        "\n",
        "class FilmDataProcessorSpark:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.dataframes = {}\n",
        "        self.spark = SparkSession.builder.master(\"local[*]\").appName(\"ExcelToSpark\")\\\n",
        "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
        "\n",
        "    def load_data(self):\n",
        "        sheet_names = ['film', 'inventory', 'rental', 'customer', 'store']\n",
        "        for sheet in sheet_names:\n",
        "            df_pandas = pd.read_excel(self.file_path, sheet_name=sheet)\n",
        "            if not df_pandas.empty:\n",
        "                df_spark = self.spark.createDataFrame(df_pandas)\n",
        "                self.dataframes[sheet] = df_spark\n",
        "\n",
        "    def clean_column_names(self, df):\n",
        "        new_column_names = [col_name.strip().replace(' ', '_').replace('.', '_') for col_name in df.columns]\n",
        "        renamed_columns = [f\"`{old_name}` AS `{new_name}`\" for old_name, new_name in zip(df.columns, new_column_names)]\n",
        "        df = df.selectExpr(*renamed_columns)\n",
        "        return df\n",
        "\n",
        "    def clean_data(self):\n",
        "        for sheet_name, df in self.dataframes.items():\n",
        "            df = self.clean_column_names(df)\n",
        "            if sheet_name == 'film':\n",
        "                df = self.clean_film(df)\n",
        "            elif sheet_name == 'inventory':\n",
        "                df = self.clean_inventory(df)\n",
        "            elif sheet_name == 'rental':\n",
        "                df = self.clean_rental(df)\n",
        "            elif sheet_name == 'customer':\n",
        "                df = self.clean_customer(df)\n",
        "            elif sheet_name == 'store':\n",
        "                df = self.clean_store(df)\n",
        "            self.dataframes[sheet_name] = df\n",
        "\n",
        "    def clean_film(self, df):\n",
        "        df = df.withColumn('film_id', regexp_replace(col('film_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('title', trim(col('title')))\n",
        "        df = df.withColumn('description', trim(col('description')))\n",
        "        df = df.withColumn('release_year', regexp_replace(col('release_year'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('language_id', regexp_replace(col('language_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('original_language_id', regexp_replace(col('original_language_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('rental_duration', regexp_replace(col('rental_duration'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('rental_rate', regexp_replace(col('rental_rate'), r'[^\\d.]', '').cast(FloatType()))\n",
        "        df = df.withColumn('length', regexp_replace(col('length'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('replacement_cost', regexp_replace(col('replacement_cost'), r'[^\\d.]', '').cast(FloatType()))\n",
        "        df = df.withColumn('rating', trim(col('rating')))\n",
        "        df = df.withColumn('special_features', trim(col('special_features')))\n",
        "        df = df.withColumn('last_update_film', to_timestamp(trim(col('last_update')), 'yyyy-MM-dd HH:mm:ss')).drop('last_update')\n",
        "        return df\n",
        "\n",
        "    def clean_inventory(self, df):\n",
        "        df = df.withColumn('inventory_id', regexp_replace(col('inventory_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('film_id', regexp_replace(col('film_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('store_id', regexp_replace(col('store_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('last_update_inventory', to_timestamp(trim(col('last_update')), 'yyyy-MM-dd HH:mm:ss')).drop('last_update')\n",
        "        return df\n",
        "\n",
        "    def clean_rental(self, df):\n",
        "        df = df.withColumn('rental_id', regexp_replace(col('rental_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('inventory_id', regexp_replace(col('inventory_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('customer_id', regexp_replace(col('customer_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('staff_id', regexp_replace(col('staff_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('rental_date', to_timestamp(trim(col('rental_date')), 'yyyy-MM-dd HH:mm:ss'))\n",
        "        df = df.withColumn('return_date', to_timestamp(trim(col('return_date')), 'yyyy-MM-dd HH:mm:ss'))\n",
        "        df = df.withColumn('last_update_rental', to_timestamp(trim(col('last_update')), 'yyyy-MM-dd HH:mm:ss')).drop('last_update')\n",
        "        return df\n",
        "\n",
        "    def clean_customer(self, df):\n",
        "        df = df.withColumn('customer_id', regexp_replace(col('customer_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('store_id', regexp_replace(col('store_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('first_name', trim(col('first_name')))\n",
        "        df = df.withColumn('last_name', trim(col('last_name')))\n",
        "        df = df.withColumn('email', trim(col('email')))\n",
        "        df = df.withColumn('address_id', regexp_replace(col('address_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('active', regexp_replace(col('active'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('create_date', to_timestamp(trim(col('create_date')), 'yyyy-MM-dd HH:mm:ss'))\n",
        "        df = df.withColumn('last_update_customer', to_timestamp(trim(col('last_update')), 'yyyy-MM-dd HH:mm:ss')).drop('last_update')\n",
        "        if 'customer_id_old' in df.columns:\n",
        "            df = df.drop('customer_id_old')\n",
        "        return df\n",
        "\n",
        "    def clean_store(self, df):\n",
        "        df = df.withColumn('store_id', regexp_replace(col('store_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('manager_staff_id', regexp_replace(col('manager_staff_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('address_id', regexp_replace(col('address_id'), r'[^0-9]', '').cast(IntegerType()))\n",
        "        df = df.withColumn('last_update_store', to_timestamp(trim(col('last_update')), 'yyyy-MM-dd HH:mm:ss')).drop('last_update')\n",
        "        return df\n",
        "\n",
        "    def combine_data(self):\n",
        "        # Unir inventory con film\n",
        "        df_inventory_film = self.dataframes['inventory'].alias('inv').join(\n",
        "            self.dataframes['film'].alias('film'),\n",
        "            on='film_id',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Unir inventory_film con store\n",
        "        df_inventory_film_store = df_inventory_film.alias('inv_film').join(\n",
        "            self.dataframes['store'].alias('store'),\n",
        "            on=col('inv_film.store_id') == col('store.store_id'),\n",
        "            how='left'\n",
        "        ).select(\n",
        "            [col('inv_film.' + c) for c in df_inventory_film.columns] +\n",
        "            [col('store.' + c).alias(c + '_store') for c in self.dataframes['store'].columns if c != 'store_id']\n",
        "        )\n",
        "\n",
        "        # Unir rental con inventory_film_store\n",
        "        df_rental_inventory_film_store = self.dataframes['rental'].alias('rental').join(\n",
        "            df_inventory_film_store.alias('inv_film_store'),\n",
        "            on='inventory_id',\n",
        "            how='left'\n",
        "        ).select(\n",
        "            [col('rental.' + c) for c in self.dataframes['rental'].columns] +\n",
        "            [col('inv_film_store.' + c) for c in df_inventory_film_store.columns if c != 'inventory_id']\n",
        "        )\n",
        "\n",
        "        # Unir rental_inventory_film_store con customer\n",
        "        df_final = df_rental_inventory_film_store.alias('rental_inv_film_store').join(\n",
        "            self.dataframes['customer'].alias('cust'),\n",
        "            on='customer_id',\n",
        "            how='left'\n",
        "        ).select(\n",
        "            [col('rental_inv_film_store.' + c) for c in df_rental_inventory_film_store.columns] +\n",
        "            [col('cust.' + c).alias(c + '_customer') for c in self.dataframes['customer'].columns if c != 'customer_id']\n",
        "        )\n",
        "\n",
        "        return df_final\n",
        "\n",
        "    def save_and_download(self, df, file_name):\n",
        "        df.toPandas().to_excel(f'{file_name}.xlsx', index=False)\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(f'{file_name}.xlsx')\n",
        "        except ImportError:\n",
        "            print(\"El entorno no es Google Colab. El archivo se ha guardado localmente.\")\n",
        "\n",
        "# Uso de la clase FilmDataProcessorSpark\n",
        "file_path = '/content/Films_2.xlsx'  # Ajustar la ruta del archivo\n",
        "film_processor = FilmDataProcessorSpark(file_path)\n",
        "\n",
        "# Cargar, limpiar, combinar y guardar los datos\n",
        "film_processor.load_data()\n",
        "film_processor.clean_data()\n",
        "df_final = film_processor.combine_data()\n",
        "df_final.printSchema()\n",
        "df_final.show()\n",
        "film_processor.save_and_download(df_final, 'Final_Combined_DataFrame')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vUDVT3zyWsCj",
        "outputId": "24c827cc-a86b-4b95-a74f-45ec8c4d3847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- rental_id: integer (nullable = true)\n",
            " |-- rental_date: timestamp (nullable = true)\n",
            " |-- inventory_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- return_date: timestamp (nullable = true)\n",
            " |-- staff_id: integer (nullable = true)\n",
            " |-- last_update_rental: timestamp (nullable = true)\n",
            " |-- film_id: integer (nullable = true)\n",
            " |-- store_id: integer (nullable = true)\n",
            " |-- last_update_inventory: timestamp (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- release_year: integer (nullable = true)\n",
            " |-- language_id: integer (nullable = true)\n",
            " |-- original_language_id: integer (nullable = true)\n",
            " |-- rental_duration: integer (nullable = true)\n",
            " |-- rental_rate: float (nullable = true)\n",
            " |-- length: integer (nullable = true)\n",
            " |-- replacement_cost: float (nullable = true)\n",
            " |-- num_voted_users: string (nullable = true)\n",
            " |-- rating: string (nullable = true)\n",
            " |-- special_features: string (nullable = true)\n",
            " |-- last_update_film: timestamp (nullable = true)\n",
            " |-- manager_staff_id_store: integer (nullable = true)\n",
            " |-- address_id_store: integer (nullable = true)\n",
            " |-- last_update_store_store: timestamp (nullable = true)\n",
            " |-- store_id_customer: integer (nullable = true)\n",
            " |-- first_name_customer: string (nullable = true)\n",
            " |-- last_name_customer: string (nullable = true)\n",
            " |-- email_customer: string (nullable = true)\n",
            " |-- address_id_customer: integer (nullable = true)\n",
            " |-- active_customer: integer (nullable = true)\n",
            " |-- create_date_customer: timestamp (nullable = true)\n",
            " |-- segment_customer: string (nullable = true)\n",
            " |-- last_update_customer_customer: timestamp (nullable = true)\n",
            "\n",
            "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+-------+--------+---------------------+-------------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+---------------+------+-----------------+-------------------+----------------------+----------------+-----------------------+-----------------+-------------------+------------------+--------------------+-------------------+---------------+--------------------+----------------+-----------------------------+\n",
            "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id| last_update_rental|film_id|store_id|last_update_inventory|              title|         description|release_year|language_id|original_language_id|rental_duration|rental_rate|length|replacement_cost|num_voted_users|rating| special_features|   last_update_film|manager_staff_id_store|address_id_store|last_update_store_store|store_id_customer|first_name_customer|last_name_customer|      email_customer|address_id_customer|active_customer|create_date_customer|segment_customer|last_update_customer_customer|\n",
            "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+-------+--------+---------------------+-------------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+---------------+------+-----------------+-------------------+----------------------+----------------+-----------------------+-----------------+-------------------+------------------+--------------------+-------------------+---------------+--------------------+----------------+-----------------------------+\n",
            "|       21|2005-05-25 01:59:46|         146|        388|2005-05-26 01:01:46|       2|2006-02-15 21:30:53|     31|       2|  2006-02-15 05:09:17|      APACHE DIVINE|A Awe-Inspiring R...|        2006|          1|                NULL|              5|       4.99|    92|           16.99|          32000| NC-17|     Commentaries|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|              CRAIG|           MORRELL|CRAIG.MORRELL@sak...|                393|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8201|2005-07-28 23:10:48|         220|        567|2005-08-01 00:50:48|       2|2006-02-15 21:30:53|     49|       2|  2006-02-15 05:09:17|        BADMAN DAWN|A Emotional Panor...|        2006|          1|                NULL|              6|       2.99|   162|           22.99|          39200|     R|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|            ALFREDO|           MCADAMS|ALFREDO.MCADAMS@s...|                573|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8214|2005-07-28 23:37:57|         352|        509|2005-08-07 00:29:57|       2|2006-02-15 21:30:53|     78|       2|  2006-02-15 05:09:17|   BLACKOUT PRIVATE|A Intrepid Yarn o...|        2006|          1|                NULL|              7|       2.99|    85|           12.99|          44550|    PG|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|               RAUL|           FORTIER|RAUL.FORTIER@saki...|                514|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8200|2005-07-28 23:10:46|         353|        185|2005-07-29 18:35:46|       1|2006-02-15 21:30:53|     78|       2|  2006-02-15 05:09:17|   BLACKOUT PRIVATE|A Intrepid Yarn o...|        2006|          1|                NULL|              7|       2.99|    85|           12.99|          44550|    PG|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|            ROBERTA|            HARPER|ROBERTA.HARPER@sa...|                189|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "|        1|2005-05-24 22:53:30|         367|        130|2005-05-26 22:04:30|       1|2006-02-15 21:30:53|     80|       2|  2006-02-15 05:09:17|    BLANKET BEVERLY|A Emotional Docum...|        2006|          1|                NULL|              7|       2.99|   148|           21.99|          17600|     G|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|          CHARLOTTE|            HUNTER|CHARLOTTE.HUNTER@...|                134|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "|       16|2005-05-25 00:43:11|         389|        316|2005-05-26 04:42:11|       2|2006-02-15 21:30:53|     86|       2|  2006-02-15 05:09:17|      BOOGIE AMELIE|A Lacklusture Cha...|        2006|          1|                NULL|              6|       4.99|   121|           11.99|          16600|     R|     Commentaries|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|             STEVEN|            CURLEY|STEVEN.CURLEY@sak...|                321|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8199|2005-07-28 23:10:25|         404|        471|2005-08-04 23:30:25|       1|2006-02-15 21:30:53|     89|       2|  2006-02-15 05:09:17|BORROWERS BEDAZZLED|A Brilliant Epist...|        2006|          1|                NULL|              7|       0.99|    63|           22.99|          39950|     G|     Commentaries|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|               DEAN|             SAUER|DEAN.SAUER@sakila...|                476|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|       17|2005-05-25 01:06:36|         830|        575|2005-05-27 00:43:36|       1|2006-02-15 21:30:53|    181|       2|  2006-02-15 05:09:17|  CONTACT ANONYMOUS|A Insightful Disp...|        2006|          1|                NULL|              7|       2.99|   166|           10.99|          72750| PG-13|     Commentaries|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|              ISAAC|           OGLESBY|ISAAC.OGLESBY@sak...|                581|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8207|2005-07-28 23:26:31|        1195|        475|2005-08-06 03:26:31|       1|2006-02-15 21:30:53|    266|       2|  2006-02-15 05:09:17|    DYNAMITE TARZAN|A Intrepid Docume...|        2006|          1|                NULL|              4|       0.99|   141|           27.99|          52950| PG-13|   Deleted Scenes|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|              PEDRO|          CHESTNUT|PEDRO.CHESTNUT@sa...|                480|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8212|2005-07-28 23:37:23|        1305|        159|2005-08-04 04:33:23|       2|2006-02-15 21:30:53|    288|       2|  2006-02-15 05:09:17|  ESCAPE METROPOLIS|A Taut Yarn of a ...|        2006|          1|                NULL|              7|       2.99|   167|           20.99|          74050|     R|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|               JILL|           HAWKINS|JILL.HAWKINS@saki...|                163|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "|     8208|2005-07-28 23:26:35|        1362|        530|2005-08-01 23:00:35|       2|2006-02-15 21:30:53|    300|       2|  2006-02-15 05:09:17|      FALCON VOLUME|A Fateful Saga of...|        2006|          1|                NULL|              5|       4.99|   102|           21.99|           8250| PG-13|     Commentaries|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|             DARRYL|          ASHCRAFT|DARRYL.ASHCRAFT@s...|                536|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|        2|2005-05-24 22:54:33|        1525|        459|2005-05-28 19:40:33|       1|2006-02-15 21:30:53|    333|       2|  2006-02-15 05:09:17|       FREAKY POCUS|A Fast-Paced Docu...|        2006|          1|                NULL|              7|       2.99|   126|           16.99|          53150|     R|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|              TOMMY|           COLLAZO|TOMMY.COLLAZO@sak...|                464|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|       12|2005-05-25 00:19:27|        1584|        261|2005-05-30 05:44:27|       2|2006-02-15 21:30:53|    347|       2|  2006-02-15 05:09:17|    GAMES BOWFINGER|A Astounding Docu...|        2006|          1|                NULL|              7|       4.99|   119|           17.99|          26950| PG-13|Behind the Scenes|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|             DEANNA|              BYRD|DEANNA.BYRD@sakil...|                266|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "|        3|2005-05-24 23:03:39|        1711|        408|2005-06-01 22:12:39|       1|2006-02-15 21:30:53|    373|       2|  2006-02-15 05:09:17|      GRADUATE LORD|A Lacklusture Epi...|        2006|          1|                NULL|              7|       2.99|   156|           14.99|          60550|     G|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|             MANUEL|           MURRELL|MANUEL.MURRELL@sa...|                413|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8198|2005-07-28 23:08:05|        1767|        356|2005-08-06 00:43:05|       2|2006-02-15 21:30:53|    383|       2|  2006-02-15 05:09:17|     GROOVE FICTION|A Unbelieveable R...|        2006|          1|                NULL|              6|       0.99|   111|           13.99|          22400| NC-17|Behind the Scenes|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|             GERALD|             FULTZ|GERALD.FULTZ@saki...|                361|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|       10|2005-05-25 00:02:21|        1824|        399|2005-05-31 22:44:21|       2|2006-02-15 21:30:53|    396|       2|  2006-02-15 05:09:17|       HANGING DEEP|A Action-Packed Y...|        2006|          1|                NULL|              5|       4.99|    62|           18.99|          45250|     G|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                1|              DANNY|              ISOM|DANNY.ISOM@sakila...|                404|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|     8211|2005-07-28 23:34:22|        1859|        193|2005-08-04 21:18:22|       1|2006-02-15 21:30:53|    406|       2|  2006-02-15 05:09:17|   HAUNTING PIANIST|A Fast-Paced Stor...|        2006|          1|                NULL|              5|       0.99|   181|           22.99|           7900|     R|Behind the Scenes|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|              KATIE|           ELLIOTT|KATIE.ELLIOTT@sak...|                197|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "|       19|2005-05-25 01:17:24|        1941|        456|2005-05-31 06:00:24|       1|2006-02-15 21:30:53|    422|       2|  2006-02-15 05:09:17|    HOLLOW JEOPARDY|A Beautiful Chara...|        2006|          1|                NULL|              7|       4.99|   136|           25.99|           7600| NC-17|Behind the Scenes|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|             RONNIE|          RICKETTS|RONNIE.RICKETTS@s...|                461|              1| 2006-02-14 22:04:37|            NULL|          2006-02-15 04:57:20|\n",
            "|        5|2005-05-24 23:05:21|        2079|        222|2005-06-02 04:33:21|       1|2006-02-15 21:30:53|    450|       2|  2006-02-15 05:09:17|    IDOLS SNATCHERS|A Insightful Dram...|        2006|          1|                NULL|              5|       2.99|    84|           29.99|          38450| NC-17|         Trailers|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|            DELORES|            HANSEN|DELORES.HANSEN@sa...|                226|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "|     8216|2005-07-28 23:43:59|        2211|        254|2005-08-06 05:05:59|       1|2006-02-15 21:30:53|    478|       2|  2006-02-15 05:09:17|         JAWS HARRY|A Thrilling Displ...|        2006|          1|                NULL|              4|       2.99|   112|           10.99|          10700|     G|   Deleted Scenes|2020-01-25 14:40:46|                     2|               2|    2016-02-15 04:57:12|                2|             MAXINE|             SILVA|MAXINE.SILVA@saki...|                259|              1| 2006-02-14 22:04:36|            NULL|          2006-02-15 04:57:20|\n",
            "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+-------+--------+---------------------+-------------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+---------------+------+-----------------+-------------------+----------------------+----------------+-----------------------+-----------------+-------------------+------------------+--------------------+-------------------+---------------+--------------------+----------------+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_821f0bc6-ddb6-4d22-99fa-90aaac9c2aa9\", \"Final_Combined_DataFrame.xlsx\", 3407385)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AKkN6gk5-pC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}